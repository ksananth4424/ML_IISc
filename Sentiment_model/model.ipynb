{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "import scipy.io\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "def relu_prime(x):\n",
    "    return np.where(x > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuadraticCost(object):\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        return 0.5 * np.linalg.norm(a-y)**2\n",
    "\n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        return (a-y) * tanh_prime(z)\n",
    "    \n",
    "class CrossEntropyCost(object):\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        return (a-y)\n",
    "    \n",
    "class Sigmoid(object):\n",
    "    @staticmethod\n",
    "    def activate(z):\n",
    "        return 1.0/(1.0+np.exp(-z))\n",
    "    \n",
    "    @staticmethod\n",
    "    def delta(z):\n",
    "        return sigmoid(z)*(1-sigmoid(z))\n",
    "    \n",
    "class Tanh(object):\n",
    "    @staticmethod\n",
    "    def activate(z):\n",
    "        return np.tanh(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def delta(z):\n",
    "        return 1.0 - np.tanh(z)**2\n",
    "    \n",
    "class ReLU(object):\n",
    "    @staticmethod\n",
    "    def activate(z):\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def delta(z):\n",
    "        return np.where(z > 0, 1, 0)\n",
    "    \n",
    "class Network(object):\n",
    "    def __init__(self, sizes, cost=CrossEntropyCost, func=Sigmoid):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.weight_initializer()\n",
    "        self.cost=cost\n",
    "        self.func=func\n",
    "\n",
    "    def weight_initializer(self):\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = (self.func).activate(np.dot(w, a)+b)\n",
    "        return a\n",
    "    \n",
    "    def train(self, training_data, epochs, learning_rate, save=False):\n",
    "        n = len(training_data)\n",
    "        log_file = open(\"log.txt\", \"w\")  # records loss for each epoch\n",
    "        log_file.write(\"Number of epochs: %d, Learning rate: %f\\n\" % (epochs, learning_rate))\n",
    "    \n",
    "        # perform gradient check using numerical approximation\n",
    "        # self.gradient_check(training_data)\n",
    "\n",
    "        # Initialize a list to store loss for each epoch\n",
    "        losses = []\n",
    "    \n",
    "        for j in range(epochs):\n",
    "            error_b = [np.zeros(b.shape) for b in self.biases]\n",
    "            error_w = [np.zeros(w.shape) for w in self.weights]\n",
    "            for (x, y) in training_data:\n",
    "                delta_error_b, delta_error_w = self.backprop(x, y)\n",
    "                error_b = [(eb+deb/n) for eb, deb in zip(error_b, delta_error_b)]\n",
    "                error_w = [(ew+dew/n) for ew, dew in zip(error_w, delta_error_w)]\n",
    "    \n",
    "            loss = self.loss(training_data)\n",
    "            log_file.write(\"Epoch %d: %f\\n\" % (j, loss))\n",
    "            log_file.flush()  # Flush the file to disk after writing each line\n",
    "            losses.append(loss)\n",
    "    \n",
    "            self.weights = [w-learning_rate*ew for w, ew in zip(self.weights, error_w)]\n",
    "            self.biases = [b-learning_rate*eb for b, eb in zip(self.biases, error_b)]                \n",
    "    \n",
    "        log_file.close()\n",
    "\n",
    "        if save:\n",
    "            self.save(\"model.txt\")\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        error_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        error_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        activation = x\n",
    "        activations = [x]\n",
    "        zs = []\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = (self.func).activate(z)\n",
    "            activations.append(activation)\n",
    "        delta = (self.cost).delta(zs[-1], activations[-1], y)\n",
    "        error_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        error_b[-1] = delta\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = (self.func).delta(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            error_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "            error_b[-l] = delta\n",
    "        return (error_b, error_w)\n",
    "    \n",
    "    def loss(self, data):\n",
    "        loss = 0\n",
    "        for x, y in data:\n",
    "            a = self.feedforward(x)\n",
    "            # if convert: y = vectorized_result(y)\n",
    "            loss += self.cost.fn(a, y)/len(data)\n",
    "        return loss\n",
    "    \n",
    "    def save(self, filename):\n",
    "        # save with the following format: sizes, weights, biases, cost, func line by line\n",
    "        with open(filename, \"w\") as f:\n",
    "            f.write(\"%s\\n\" % self.sizes)\n",
    "            for w in self.weights:\n",
    "                for row in w:\n",
    "                    f.write(\"%s\\n\" % row)\n",
    "            for b in self.biases:\n",
    "                for row in b:\n",
    "                    f.write(\"%s\\n\" % row)\n",
    "            f.write(\"%s\\n\" % self.cost)\n",
    "            f.write(\"%s\\n\" % self.func)\n",
    "        f.close()\n",
    "\n",
    "    def test(self, test_data, error=False):\n",
    "        # print(self.feedforward(test_data[0][0]))\n",
    "        test_results = [(1 if self.feedforward(x) > 0 else -1, y) for (x, y) in test_data]\n",
    "        accuracy = np.mean([int(x == y) for (x, y) in test_results])\n",
    "        # print(\"Accuracy: \", accuracy)\n",
    "        # print(test_results)\n",
    "        if error:\n",
    "            return 1 - accuracy\n",
    "        return accuracy\n",
    "\n",
    "    \n",
    "    def gradient_check(self, training_data):\n",
    "        epsilon = 1e-7\n",
    "        tolerance = 1e-3\n",
    "        \n",
    "        for x, y in training_data:\n",
    "            backprop_b, backprop_w = self.backprop(x, y)\n",
    "\n",
    "            for i in range(len(self.biases)):\n",
    "                for j in range(len(self.biases[i])):\n",
    "                    original_bias = self.biases[i][j]\n",
    "                    self.biases[i][j] += epsilon\n",
    "                    loss_plus = self.loss([(x, y)])\n",
    "                    self.biases[i][j] -= 2*epsilon\n",
    "                    loss_minus = self.loss([(x, y)])\n",
    "                    self.biases[i][j] = original_bias\n",
    "                    numerical_gradient = (loss_plus - loss_minus) / (2*epsilon)\n",
    "\n",
    "                    if abs(numerical_gradient - backprop_b[i][j]) > tolerance:\n",
    "                        print(\"Gradient check failed for bias[%d][%d]: %f vs %f\" % (i, j, numerical_gradient, backprop_b[i][j]))\n",
    "\n",
    "            for i in range(len(self.weights)):\n",
    "                for j in range(len(self.weights[i])):\n",
    "                    for k in range(len(self.weights[i][j])):\n",
    "                        original_weight = self.weights[i][j][k]\n",
    "                        self.weights[i][j][k] += epsilon\n",
    "                        loss_plus = self.loss([(x, y)])\n",
    "                        self.weights[i][j][k] -= 2*epsilon\n",
    "                        loss_minus = self.loss([(x, y)])\n",
    "                        self.weights[i][j][k] = original_weight\n",
    "                        numerical_gradient = (loss_plus - loss_minus) / (2*epsilon)\n",
    "\n",
    "                        if abs(numerical_gradient - backprop_w[i][j][k]) > tolerance:\n",
    "                            print(\"Gradient check failed for weight[%d][%d][%d]: %f vs %f\" % (i, j, k, numerical_gradient, backprop_w[i][j][k]))\n",
    "\n",
    "def vectorized_result(j):\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_network():\n",
    "    file = open(\"network.txt\", \"r\")\n",
    "    # format: sizes, cost function, activation function each on a separate line \n",
    "    sizes = list(map(int, file.readline().split()))\n",
    "\n",
    "    # Read the cost function and activation function names\n",
    "    cost_fn_name = file.readline().strip()\n",
    "    activation_fn_name = file.readline().strip()\n",
    "\n",
    "    # Mapping of cost function names to classes\n",
    "    cost_fn_mapping = {\n",
    "        \"QuadraticCost\": QuadraticCost,\n",
    "        \"CrossEntropyCost\": CrossEntropyCost\n",
    "    }\n",
    "\n",
    "    # Mapping of activation function names to classes\n",
    "    activation_fn_mapping = {\n",
    "        \"Sigmoid\": Sigmoid,\n",
    "        \"Tanh\": Tanh\n",
    "    }\n",
    "\n",
    "    # Get the actual classes based on the names read from the file\n",
    "    cost_fn = cost_fn_mapping.get(cost_fn_name, CrossEntropyCost)  # Default to CrossEntropyCost if not found\n",
    "    activation_fn = activation_fn_mapping.get(activation_fn_name, Sigmoid)  # Default to Sigmoid if not found\n",
    "\n",
    "    # Initialize the network with the correct classes\n",
    "    net = Network(sizes, cost=cost_fn, func=activation_fn)\n",
    "    print(sizes, cost_fn_name, activation_fn_name)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word to Vector\n",
    "\n",
    "In order to download the word2vec file go to https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g and download teh file which is 1.5GB!. Then place the downloaded file in the current directory and the proceed with the execution of the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained Word2Vec model\n",
    "model_path = './GoogleNews-vectors-negative300.bin.gz'\n",
    "w2v_model = KeyedVectors.load_word2vec_format(model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_of_sentence(sentence, model):\n",
    "    words = sentence.split()\n",
    "    vector = np.zeros(model.vector_size)\n",
    "    valid_words = 0\n",
    "    for word in words:\n",
    "        if word in model.key_to_index:  # Use .key_to_index for checking if a word is in the model\n",
    "            vector += model[word]\n",
    "            valid_words += 1\n",
    "    if valid_words:\n",
    "        vector /= valid_words\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load SST2 Dataset and make the required adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"stanfordnlp/sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "combined_dataset = concatenate_datasets([ds['train'], ds['validation'], ds['test']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(item['sentence'], item['label']) for item in combined_dataset]\n",
    "X_train, X_test, y_train, y_test = train_test_split([x[0] for x in data], [x[1] for x in data], test_size=0.2, random_state=42)\n",
    "training_data = [((x), int(y)) for x, y in zip(X_train, y_train)]\n",
    "test_data = [((x), int(y)) for x, y in zip(X_test, y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_vector_of_sentence(training_data[0][0], w2v_model).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
