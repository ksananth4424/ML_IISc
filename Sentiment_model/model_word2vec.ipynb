{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "import scipy.io\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "def relu_prime(x):\n",
    "    return np.where(x > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuadraticCost(object):\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        return 0.5 * np.linalg.norm(a-y)**2\n",
    "\n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        return (a-y) * sigmoid_prime(z)\n",
    "    \n",
    "class CrossEntropyCost(object):\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        return (a-y)\n",
    "    \n",
    "class Sigmoid(object):\n",
    "    @staticmethod\n",
    "    def activate(z):\n",
    "        return 1.0/(1.0+np.exp(-z))\n",
    "    \n",
    "    @staticmethod\n",
    "    def delta(z):\n",
    "        return sigmoid(z)*(1-sigmoid(z))\n",
    "    \n",
    "class Tanh(object):\n",
    "    @staticmethod\n",
    "    def activate(z):\n",
    "        return np.tanh(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def delta(z):\n",
    "        return 1.0 - np.tanh(z)**2\n",
    "    \n",
    "class ReLU(object):\n",
    "    @staticmethod\n",
    "    def activate(z):\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def delta(z):\n",
    "        return np.where(z > 0, 1, 0)\n",
    "    \n",
    "class Network(object):\n",
    "    def __init__(self, sizes, cost=CrossEntropyCost, func=Sigmoid):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.weight_initializer()\n",
    "        self.cost=cost\n",
    "        self.func=func\n",
    "\n",
    "    def weight_initializer(self):\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = (self.func).activate(np.dot(w, a)+b)\n",
    "        return a\n",
    "    \n",
    "    def train(self, training_data, epochs, learning_rate, save=False):\n",
    "        n = len(training_data)\n",
    "        log_file = open(\"log.txt\", \"w\")  # records loss for each epoch\n",
    "        log_file.write(\"Number of epochs: %d, Learning rate: %f\\n\" % (epochs, learning_rate))\n",
    "    \n",
    "        # perform gradient check using numerical approximation\n",
    "        # self.gradient_check(training_data)\n",
    "\n",
    "        # Initialize a list to store loss for each epoch\n",
    "        losses = []\n",
    "    \n",
    "        for j in range(epochs):\n",
    "            error_b = [np.zeros(b.shape) for b in self.biases]\n",
    "            error_w = [np.zeros(w.shape) for w in self.weights]\n",
    "            for (x, y) in training_data:\n",
    "                delta_error_b, delta_error_w = self.backprop(x, y)\n",
    "                error_b = [(eb+deb/n) for eb, deb in zip(error_b, delta_error_b)]\n",
    "                error_w = [(ew+dew/n) for ew, dew in zip(error_w, delta_error_w)]\n",
    "    \n",
    "            loss = self.loss(training_data)\n",
    "            log_file.write(\"Epoch %d: %f\\n\" % (j, loss))\n",
    "            log_file.flush()  # Flush the file to disk after writing each line\n",
    "            losses.append(loss)\n",
    "    \n",
    "            self.weights = [w-learning_rate*ew for w, ew in zip(self.weights, error_w)]\n",
    "            self.biases = [b-learning_rate*eb for b, eb in zip(self.biases, error_b)]                \n",
    "    \n",
    "        log_file.close()\n",
    "\n",
    "        if save:\n",
    "            self.save(\"model.txt\")\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        error_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        error_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        activation = x\n",
    "        activations = [x]\n",
    "        zs = []\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = (self.func).activate(z)\n",
    "            activations.append(activation)\n",
    "        delta = (self.cost).delta(zs[-1], activations[-1], y)\n",
    "        error_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        error_b[-1] = delta\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = (self.func).delta(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            error_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "            error_b[-l] = delta\n",
    "        return (error_b, error_w)\n",
    "    \n",
    "    def loss(self, data):\n",
    "        loss = 0\n",
    "        for x, y in data:\n",
    "            a = self.feedforward(x)\n",
    "            # if convert: y = vectorized_result(y)\n",
    "            loss += self.cost.fn(a, y)/len(data)\n",
    "        return loss\n",
    "    \n",
    "    def save(self, filename):\n",
    "        # save with the following format: sizes, weights, biases, cost, func line by line\n",
    "        with open(filename, \"w\") as f:\n",
    "            f.write(\"%s\\n\" % self.sizes)\n",
    "            for w in self.weights:\n",
    "                for row in w:\n",
    "                    f.write(\"%s\\n\" % row)\n",
    "            for b in self.biases:\n",
    "                for row in b:\n",
    "                    f.write(\"%s\\n\" % row)\n",
    "            f.write(\"%s\\n\" % self.cost)\n",
    "            f.write(\"%s\\n\" % self.func)\n",
    "        f.close()\n",
    "\n",
    "    def test(self, test_data, error=False):\n",
    "        # print(self.feedforward(test_data[0][0]))\n",
    "        test_results = [(1 if self.feedforward(x) > 0 else -1, y) for (x, y) in test_data]\n",
    "        accuracy = np.mean([int(x == y) for (x, y) in test_results])\n",
    "        # print(\"Accuracy: \", accuracy)\n",
    "        # print(test_results)\n",
    "        if error:\n",
    "            return 1 - accuracy\n",
    "        return accuracy\n",
    "\n",
    "    \n",
    "    def gradient_check(self, training_data):\n",
    "        epsilon = 1e-7\n",
    "        tolerance = 1e-3\n",
    "        \n",
    "        for x, y in training_data:\n",
    "            backprop_b, backprop_w = self.backprop(x, y)\n",
    "\n",
    "            for i in range(len(self.biases)):\n",
    "                for j in range(len(self.biases[i])):\n",
    "                    original_bias = self.biases[i][j]\n",
    "                    self.biases[i][j] += epsilon\n",
    "                    loss_plus = self.loss([(x, y)])\n",
    "                    self.biases[i][j] -= 2*epsilon\n",
    "                    loss_minus = self.loss([(x, y)])\n",
    "                    self.biases[i][j] = original_bias\n",
    "                    numerical_gradient = (loss_plus - loss_minus) / (2*epsilon)\n",
    "\n",
    "                    if abs(numerical_gradient - backprop_b[i][j]) > tolerance:\n",
    "                        print(\"Gradient check failed for bias[%d][%d]: %f vs %f\" % (i, j, numerical_gradient, backprop_b[i][j]))\n",
    "\n",
    "            for i in range(len(self.weights)):\n",
    "                for j in range(len(self.weights[i])):\n",
    "                    for k in range(len(self.weights[i][j])):\n",
    "                        original_weight = self.weights[i][j][k]\n",
    "                        self.weights[i][j][k] += epsilon\n",
    "                        loss_plus = self.loss([(x, y)])\n",
    "                        self.weights[i][j][k] -= 2*epsilon\n",
    "                        loss_minus = self.loss([(x, y)])\n",
    "                        self.weights[i][j][k] = original_weight\n",
    "                        numerical_gradient = (loss_plus - loss_minus) / (2*epsilon)\n",
    "\n",
    "                        if abs(numerical_gradient - backprop_w[i][j][k]) > tolerance:\n",
    "                            print(\"Gradient check failed for weight[%d][%d][%d]: %f vs %f\" % (i, j, k, numerical_gradient, backprop_w[i][j][k]))\n",
    "\n",
    "def vectorized_result(j):\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_network():\n",
    "    file = open(\"network.txt\", \"r\")\n",
    "    # format: sizes, cost function, activation function each on a separate line \n",
    "    sizes = list(map(int, file.readline().split()))\n",
    "\n",
    "    # Read the cost function and activation function names\n",
    "    cost_fn_name = file.readline().strip()\n",
    "    activation_fn_name = file.readline().strip()\n",
    "\n",
    "    # Mapping of cost function names to classes\n",
    "    cost_fn_mapping = {\n",
    "        \"QuadraticCost\": QuadraticCost,\n",
    "        \"CrossEntropyCost\": CrossEntropyCost\n",
    "    }\n",
    "\n",
    "    # Mapping of activation function names to classes\n",
    "    activation_fn_mapping = {\n",
    "        \"Sigmoid\": Sigmoid,\n",
    "        \"Tanh\": Tanh\n",
    "    }\n",
    "\n",
    "    # Get the actual classes based on the names read from the file\n",
    "    cost_fn = cost_fn_mapping.get(cost_fn_name, CrossEntropyCost)  # Default to CrossEntropyCost if not found\n",
    "    activation_fn = activation_fn_mapping.get(activation_fn_name, Sigmoid)  # Default to Sigmoid if not found\n",
    "\n",
    "    # Initialize the network with the correct classes\n",
    "    net = Network(sizes, cost=cost_fn, func=activation_fn)\n",
    "    print(sizes, cost_fn_name, activation_fn_name)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word to Vector\n",
    "\n",
    "In order to download the word2vec file go to https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g and download teh file which is 1.5GB!. Then place the downloaded file in the current directory and the proceed with the execution of the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained Word2Vec model\n",
    "model_path = './GoogleNews-vectors-negative300.bin.gz'\n",
    "w2v_model = KeyedVectors.load_word2vec_format(model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_of_sentence(sentence, model):\n",
    "    words = sentence.split()\n",
    "    vector = np.zeros(model.vector_size)\n",
    "    valid_words = 0\n",
    "    for word in words:\n",
    "        if word in model.key_to_index:  # Use .key_to_index for checking if a word is in the model\n",
    "            vector += model[word]\n",
    "            valid_words += 1\n",
    "    if valid_words:\n",
    "        vector /= valid_words\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load SST2 Dataset and make the required adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"stanfordnlp/sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "combined_dataset = concatenate_datasets([ds['train'], ds['validation'], ds['test']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(item['sentence'], item['label']) for item in combined_dataset]\n",
    "X_train_sentence, X_test_sentence, y_train, y_test = train_test_split([x[0] for x in data], [x[1] for x in data], test_size=0.2, random_state=42)\n",
    "X_train = [get_vector_of_sentence(sentence, w2v_model) for sentence in X_train_sentence]\n",
    "X_test = [get_vector_of_sentence(sentence, w2v_model) for sentence in X_test_sentence]\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "training_data = [((x), int(y)) for x, y in zip(X_train, y_train)]\n",
    "test_data = [((x), int(y)) for x, y in zip(X_test, y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56033"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300, 50, 1] QuadraticCost Sigmoid\n"
     ]
    }
   ],
   "source": [
    "net = load_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m eta \u001b[38;5;241m=\u001b[39m initial_eta \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m decay \u001b[38;5;241m*\u001b[39m iterations)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# eta = initial_eta * decay_rate ** (iterations / decay_step)\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# net.train(training_data, 1, eta)\u001b[39;00m\n\u001b[0;32m     29\u001b[0m temp \u001b[38;5;241m=\u001b[39m Network\u001b[38;5;241m.\u001b[39mloss(net, training_data)\n",
      "Cell \u001b[1;32mIn[4], line 82\u001b[0m, in \u001b[0;36mNetwork.train\u001b[1;34m(self, training_data, epochs, learning_rate, save)\u001b[0m\n\u001b[0;32m     79\u001b[0m     error_b \u001b[38;5;241m=\u001b[39m [(eb\u001b[38;5;241m+\u001b[39mdeb\u001b[38;5;241m/\u001b[39mn) \u001b[38;5;28;01mfor\u001b[39;00m eb, deb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(error_b, delta_error_b)]\n\u001b[0;32m     80\u001b[0m     error_w \u001b[38;5;241m=\u001b[39m [(ew\u001b[38;5;241m+\u001b[39mdew\u001b[38;5;241m/\u001b[39mn) \u001b[38;5;28;01mfor\u001b[39;00m ew, dew \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(error_w, delta_error_w)]\n\u001b[1;32m---> 82\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m log_file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (j, loss))\n\u001b[0;32m     84\u001b[0m log_file\u001b[38;5;241m.\u001b[39mflush()  \u001b[38;5;66;03m# Flush the file to disk after writing each line\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 120\u001b[0m, in \u001b[0;36mNetwork.loss\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    118\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[1;32m--> 120\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeedforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# if convert: y = vectorized_result(y)\u001b[39;00m\n\u001b[0;32m    122\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcost\u001b[38;5;241m.\u001b[39mfn(a, y)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(data)\n",
      "Cell \u001b[1;32mIn[4], line 60\u001b[0m, in \u001b[0;36mNetwork.feedforward\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeedforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, a):\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m b, w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights):\n\u001b[1;32m---> 60\u001b[0m         a \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc)\u001b[38;5;241m.\u001b[39mactivate(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m+\u001b[39mb)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n = len(training_data)\n",
    "k = n//10\n",
    "\n",
    "initial_eta = 0.01\n",
    "decay_rate = 0.96\n",
    "decay_step = 100\n",
    "decay = 0.002\n",
    "\n",
    "iterations = 1\n",
    "losses = []\n",
    "previous_loss = 0\n",
    "iteration_time = []\n",
    "# np.random.seed(42)\n",
    "while True:\n",
    "    time1 = time.process_time()\n",
    "    random_seed = random.randint(0, n)\n",
    "    np.random.seed(random_seed)\n",
    "    indices = np.random.choice(n, k, replace=True)\n",
    "    temp_X_train = X_train[indices]\n",
    "    temp_y_train = y_train[indices]\n",
    "    training_inputs = [np.reshape(x, (len(x), 1)) for x in temp_X_train]\n",
    "    training_results = [np.reshape(y, (1, 1)) for y in temp_y_train]\n",
    "    training_data = list(zip(training_inputs, training_results))\n",
    "\n",
    "    eta = initial_eta / (1 + decay * iterations)\n",
    "    # eta = initial_eta * decay_rate ** (iterations / decay_step)\n",
    "    net.train(training_data, 1, eta)\n",
    "    # net.train(training_data, 1, eta)\n",
    "    temp = Network.loss(net, training_data)\n",
    "    losses.append(temp)\n",
    "    # if abs(temp - previous_loss) < 1e-7 or iterations == 8000:\n",
    "    #     break\n",
    "    if(iterations == 2000):\n",
    "       break\n",
    "    previous_loss = temp\n",
    "    iterations += 1\n",
    "    time2 = time.process_time()\n",
    "    iteration_time.append(time2 - time1)\n",
    "\n",
    "print(\"Number of iterations: \", iterations)\n",
    "print(\"Average iteration time: \", np.mean(iteration_time))\n",
    "print(\"Total time: \", np.sum(iteration_time))\n",
    "print(net.test(test_data))\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
