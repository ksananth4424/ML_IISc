{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "import scipy.io\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "def relu_prime(x):\n",
    "    return np.where(x > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuadraticCost(object):\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        return 0.5 * np.linalg.norm(a-y)**2\n",
    "\n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        return (a-y) * sigmoid_prime(z)\n",
    "    \n",
    "class CrossEntropyCost(object):\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        return (a-y)\n",
    "    \n",
    "class Sigmoid(object):\n",
    "    @staticmethod\n",
    "    def activate(z):\n",
    "        return 1.0/(1.0+np.exp(-z))\n",
    "    \n",
    "    @staticmethod\n",
    "    def delta(z):\n",
    "        return sigmoid(z)*(1-sigmoid(z))\n",
    "    \n",
    "class Tanh(object):\n",
    "    @staticmethod\n",
    "    def activate(z):\n",
    "        return np.tanh(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def delta(z):\n",
    "        return 1.0 - np.tanh(z)**2\n",
    "    \n",
    "class ReLU(object):\n",
    "    @staticmethod\n",
    "    def activate(z):\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def delta(z):\n",
    "        return np.where(z > 0, 1, 0)\n",
    "    \n",
    "class Network(object):\n",
    "    def __init__(self, sizes, cost=CrossEntropyCost, func=Sigmoid):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.weight_initializer()\n",
    "        self.cost=cost\n",
    "        self.func=func\n",
    "\n",
    "    def weight_initializer(self):\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = (self.func).activate(np.dot(w, a)+b)\n",
    "        return a\n",
    "    \n",
    "    def train(self, training_data, epochs, learning_rate, save=False):\n",
    "        n = len(training_data)\n",
    "        log_file = open(\"log.txt\", \"w\")  # records loss for each epoch\n",
    "        log_file.write(\"Number of epochs: %d, Learning rate: %f\\n\" % (epochs, learning_rate))\n",
    "    \n",
    "        # perform gradient check using numerical approximation\n",
    "        # self.gradient_check(training_data)\n",
    "\n",
    "        # Initialize a list to store loss for each epoch\n",
    "        losses = []\n",
    "    \n",
    "        for j in range(epochs):\n",
    "            error_b = [np.zeros(b.shape) for b in self.biases]\n",
    "            error_w = [np.zeros(w.shape) for w in self.weights]\n",
    "            for (x, y) in training_data:\n",
    "                delta_error_b, delta_error_w = self.backprop(x, y)\n",
    "                error_b = [(eb+deb/n) for eb, deb in zip(error_b, delta_error_b)]\n",
    "                error_w = [(ew+dew/n) for ew, dew in zip(error_w, delta_error_w)]\n",
    "    \n",
    "            loss = self.loss(training_data)\n",
    "            log_file.write(\"Epoch %d: %f\\n\" % (j, loss))\n",
    "            log_file.flush()  # Flush the file to disk after writing each line\n",
    "            losses.append(loss)\n",
    "    \n",
    "            self.weights = [w-learning_rate*ew for w, ew in zip(self.weights, error_w)]\n",
    "            self.biases = [b-learning_rate*eb for b, eb in zip(self.biases, error_b)]                \n",
    "    \n",
    "        log_file.close()\n",
    "\n",
    "        if save:\n",
    "            self.save(\"model.txt\")\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        error_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        error_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        activation = x\n",
    "        activations = [x]\n",
    "        zs = []\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = (self.func).activate(z)\n",
    "            activations.append(activation)\n",
    "        delta = (self.cost).delta(zs[-1], activations[-1], y)\n",
    "        error_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        error_b[-1] = delta\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = (self.func).delta(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            error_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "            error_b[-l] = delta\n",
    "        return (error_b, error_w)\n",
    "    \n",
    "    def loss(self, data):\n",
    "        loss = 0\n",
    "        for x, y in data:\n",
    "            a = self.feedforward(x)\n",
    "            # if convert: y = vectorized_result(y)\n",
    "            loss += self.cost.fn(a, y)/len(data)\n",
    "        return loss\n",
    "    \n",
    "    def save(self, filename):\n",
    "        # save with the following format: sizes, weights, biases, cost, func line by line\n",
    "        with open(filename, \"w\") as f:\n",
    "            f.write(\"%s\\n\" % self.sizes)\n",
    "            for w in self.weights:\n",
    "                for row in w:\n",
    "                    f.write(\"%s\\n\" % row)\n",
    "            for b in self.biases:\n",
    "                for row in b:\n",
    "                    f.write(\"%s\\n\" % row)\n",
    "            f.write(\"%s\\n\" % self.cost)\n",
    "            f.write(\"%s\\n\" % self.func)\n",
    "        f.close()\n",
    "\n",
    "    def test(self, test_data, error=False):\n",
    "        # print(self.feedforward(test_data[0][0]))\n",
    "        test_results = [(1 if self.feedforward(x) > 0.5 else 0, y) for (x, y) in test_data]\n",
    "        accuracy = np.mean([int(x == y) for (x, y) in test_results])\n",
    "        # print(\"Accuracy: \", accuracy)\n",
    "        # print(test_results)\n",
    "        if error:\n",
    "            return 1 - accuracy\n",
    "        return accuracy\n",
    "\n",
    "    \n",
    "    def gradient_check(self, training_data):\n",
    "        epsilon = 1e-7\n",
    "        tolerance = 1e-3\n",
    "        \n",
    "        for x, y in training_data:\n",
    "            backprop_b, backprop_w = self.backprop(x, y)\n",
    "\n",
    "            for i in range(len(self.biases)):\n",
    "                for j in range(len(self.biases[i])):\n",
    "                    original_bias = self.biases[i][j]\n",
    "                    self.biases[i][j] += epsilon\n",
    "                    loss_plus = self.loss([(x, y)])\n",
    "                    self.biases[i][j] -= 2*epsilon\n",
    "                    loss_minus = self.loss([(x, y)])\n",
    "                    self.biases[i][j] = original_bias\n",
    "                    numerical_gradient = (loss_plus - loss_minus) / (2*epsilon)\n",
    "\n",
    "                    if abs(numerical_gradient - backprop_b[i][j]) > tolerance:\n",
    "                        print(\"Gradient check failed for bias[%d][%d]: %f vs %f\" % (i, j, numerical_gradient, backprop_b[i][j]))\n",
    "\n",
    "            for i in range(len(self.weights)):\n",
    "                for j in range(len(self.weights[i])):\n",
    "                    for k in range(len(self.weights[i][j])):\n",
    "                        original_weight = self.weights[i][j][k]\n",
    "                        self.weights[i][j][k] += epsilon\n",
    "                        loss_plus = self.loss([(x, y)])\n",
    "                        self.weights[i][j][k] -= 2*epsilon\n",
    "                        loss_minus = self.loss([(x, y)])\n",
    "                        self.weights[i][j][k] = original_weight\n",
    "                        numerical_gradient = (loss_plus - loss_minus) / (2*epsilon)\n",
    "\n",
    "                        if abs(numerical_gradient - backprop_w[i][j][k]) > tolerance:\n",
    "                            print(\"Gradient check failed for weight[%d][%d][%d]: %f vs %f\" % (i, j, k, numerical_gradient, backprop_w[i][j][k]))\n",
    "\n",
    "def vectorized_result(j):\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_network():\n",
    "    file = open(\"network.txt\", \"r\")\n",
    "    # format: sizes, cost function, activation function each on a separate line \n",
    "    sizes = list(map(int, file.readline().split()))\n",
    "\n",
    "    # Read the cost function and activation function names\n",
    "    cost_fn_name = file.readline().strip()\n",
    "    activation_fn_name = file.readline().strip()\n",
    "\n",
    "    # Mapping of cost function names to classes\n",
    "    cost_fn_mapping = {\n",
    "        \"QuadraticCost\": QuadraticCost,\n",
    "        \"CrossEntropyCost\": CrossEntropyCost\n",
    "    }\n",
    "\n",
    "    # Mapping of activation function names to classes\n",
    "    activation_fn_mapping = {\n",
    "        \"Sigmoid\": Sigmoid,\n",
    "        \"Tanh\": Tanh\n",
    "    }\n",
    "\n",
    "    # Get the actual classes based on the names read from the file\n",
    "    cost_fn = cost_fn_mapping.get(cost_fn_name, CrossEntropyCost)  # Default to CrossEntropyCost if not found\n",
    "    activation_fn = activation_fn_mapping.get(activation_fn_name, Sigmoid)  # Default to Sigmoid if not found\n",
    "\n",
    "    # Initialize the network with the correct classes\n",
    "    net = Network(sizes, cost=cost_fn, func=activation_fn)\n",
    "    print(sizes, cost_fn_name, activation_fn_name)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe\n",
    "\n",
    "In order to download the GloVe file go to https://nlp.stanford.edu/projects/glove/ and download the file Wikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab, uncased, 50d, 100d, 200d, & 300d vectors, 822 MB download). Then place the downloaded file in the current directory and the proceed with the execution of the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\K S Ananth\\AppData\\Local\\Temp\\ipykernel_9116\\3724047408.py:3: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(glove_input_file, word2vec_output_file)\n"
     ]
    }
   ],
   "source": [
    "glove_input_file = './glove.6B.50d.txt'  # Adjust the path and filename as necessary\n",
    "word2vec_output_file = 'glove.6B.50d.word2vec.txt'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "\n",
    "# Load the converted vectors\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_of_sentence(sentence, model):\n",
    "    words = sentence.split()\n",
    "    vector = np.zeros(model.vector_size)\n",
    "    valid_words = 0\n",
    "    for word in words:\n",
    "        if word in model.key_to_index:  # Use .key_to_index for checking if a word is in the model\n",
    "            vector += model[word]\n",
    "            valid_words += 1\n",
    "    if valid_words:\n",
    "        vector /= valid_words\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load SST2 Dataset and make the required adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"stanfordnlp/sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "combined_dataset = concatenate_datasets([ds['train'], ds['validation'], ds['test']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(item['sentence'], item['label']) for item in combined_dataset]\n",
    "X_train_sentence, X_test_sentence, y_train, y_test = train_test_split([x[0] for x in data], [x[1] for x in data], test_size=0.2, random_state=42)\n",
    "X_train = [get_vector_of_sentence(sentence, model) for sentence in X_train_sentence]\n",
    "X_test = [get_vector_of_sentence(sentence, model) for sentence in X_test_sentence]\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "training_data = [((x), int(y)) for x, y in zip(X_train, y_train)]\n",
    "test_data = [((x), int(y)) for x, y in zip(X_test, y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56033"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50, 10, 5, 1] QuadraticCost Sigmoid\n"
     ]
    }
   ],
   "source": [
    "net = load_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations:  20\n",
      "Average iteration time:  0.3675986842105263\n",
      "Total time:  6.984375\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 43\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage iteration time: \u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mmean(iteration_time))\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal time: \u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39msum(iteration_time))\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     44\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure()\n\u001b[0;32m     45\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(losses)\n",
      "Cell \u001b[1;32mIn[46], line 141\u001b[0m, in \u001b[0;36mNetwork.test\u001b[1;34m(self, test_data, error)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest\u001b[39m(\u001b[38;5;28mself\u001b[39m, test_data, error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;66;03m# print(self.feedforward(test_data[0][0]))\u001b[39;00m\n\u001b[1;32m--> 141\u001b[0m     test_results \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeedforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    142\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean([\u001b[38;5;28mint\u001b[39m(x \u001b[38;5;241m==\u001b[39m y) \u001b[38;5;28;01mfor\u001b[39;00m (x, y) \u001b[38;5;129;01min\u001b[39;00m test_results])\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;66;03m# print(\"Accuracy: \", accuracy)\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;66;03m# print(test_results)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[46], line 141\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest\u001b[39m(\u001b[38;5;28mself\u001b[39m, test_data, error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;66;03m# print(self.feedforward(test_data[0][0]))\u001b[39;00m\n\u001b[1;32m--> 141\u001b[0m     test_results \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeedforward(x) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m, y) \u001b[38;5;28;01mfor\u001b[39;00m (x, y) \u001b[38;5;129;01min\u001b[39;00m test_data]\n\u001b[0;32m    142\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean([\u001b[38;5;28mint\u001b[39m(x \u001b[38;5;241m==\u001b[39m y) \u001b[38;5;28;01mfor\u001b[39;00m (x, y) \u001b[38;5;129;01min\u001b[39;00m test_results])\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;66;03m# print(\"Accuracy: \", accuracy)\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;66;03m# print(test_results)\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "n = len(training_data)\n",
    "k = n//10\n",
    "\n",
    "initial_eta = 0.01\n",
    "decay_rate = 0.96\n",
    "decay_step = 100\n",
    "decay = 0.002\n",
    "\n",
    "iterations = 1\n",
    "losses = []\n",
    "previous_loss = 0\n",
    "iteration_time = []\n",
    "# np.random.seed(42)\n",
    "while True:\n",
    "    time1 = time.process_time()\n",
    "    random_seed = random.randint(0, n)\n",
    "    np.random.seed(random_seed)\n",
    "    indices = np.random.choice(n, k, replace=True)\n",
    "    temp_X_train = X_train[indices]\n",
    "    temp_y_train = y_train[indices]\n",
    "    training_inputs = [np.reshape(x, (len(x), 1)) for x in temp_X_train]\n",
    "    training_results = [np.reshape(y, (1, 1)) for y in temp_y_train]\n",
    "    temp_training_data = list(zip(training_inputs, training_results))\n",
    "\n",
    "    eta = initial_eta / (1 + decay * iterations)\n",
    "    # eta = initial_eta * decay_rate ** (iterations / decay_step)\n",
    "    net.train(temp_training_data, 1, eta)\n",
    "    # net.train(training_data, 1, eta)\n",
    "    temp = Network.loss(net, temp_training_data)\n",
    "    losses.append(temp)\n",
    "    # if abs(temp - previous_loss) < 1e-7 or iterations == 8000:\n",
    "    #     break\n",
    "    if(iterations == 20):\n",
    "       break\n",
    "    previous_loss = temp\n",
    "    iterations += 1\n",
    "    time2 = time.process_time()\n",
    "    iteration_time.append(time2 - time1)\n",
    "\n",
    "print(\"Number of iterations: \", iterations)\n",
    "print(\"Average iteration time: \", np.mean(iteration_time))\n",
    "print(\"Total time: \", np.sum(iteration_time))\n",
    "print(net.test(test_data))\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 50)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.feedforward(X_test[0][0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
