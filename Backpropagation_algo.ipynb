{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ef38eb4-bf19-4f0c-9aff-47a1b0a65023",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4636ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "262328b3-3428-438e-9a9d-b7f4fcbe5a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuadraticCost(object):\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        return 0.5 * np.linalg.norm(a-y)**2\n",
    "\n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        return (a-y) * sigmoid_prime(z)\n",
    "    \n",
    "class CrossEntropyCost(object):\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        return (a-y)\n",
    "    \n",
    "class Sigmoid(object):\n",
    "    @staticmethod\n",
    "    def activate(z):\n",
    "        return 1.0/(1.0+np.exp(-z))\n",
    "    \n",
    "    @staticmethod\n",
    "    def delta(z):\n",
    "        return sigmoid(z)*(1-sigmoid(z))\n",
    "    \n",
    "class Tanh(object):\n",
    "    @staticmethod\n",
    "    def activate(z):\n",
    "        return np.tanh(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def delta(z):\n",
    "        return 1 - np.tanh(z)**2\n",
    "    \n",
    "class Network(object):\n",
    "    def __init__(self, sizes, cost=CrossEntropyCost, func=Sigmoid):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.weight_initializer()\n",
    "        self.cost=cost\n",
    "        self.func=func\n",
    "\n",
    "    def weight_initializer(self):\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = (self.func).activate(np.dot(w, a)+b)\n",
    "        return a\n",
    "    \n",
    "    def train(self, training_data, epochs, learning_rate, save=False):\n",
    "        n = len(training_data)\n",
    "        log_file = open(\"log.txt\", \"w\")  # records loss for each epoch\n",
    "        log_file.write(\"Number of epochs: %d, Learning rate: %f\\n\" % (epochs, learning_rate))\n",
    "    \n",
    "        # perform gradient check using numerical approximation\n",
    "        self.gradient_check(training_data)\n",
    "\n",
    "        # Initialize a list to store loss for each epoch\n",
    "        losses = []\n",
    "    \n",
    "        for j in range(epochs):\n",
    "            error_b = [np.zeros(b.shape) for b in self.biases]\n",
    "            error_w = [np.zeros(w.shape) for w in self.weights]\n",
    "            for (x, y) in training_data:\n",
    "                delta_error_b, delta_error_w = self.backprop(x, y)\n",
    "                error_b = [(eb+deb/n) for eb, deb in zip(error_b, delta_error_b)]\n",
    "                error_w = [(ew+dew/n) for ew, dew in zip(error_w, delta_error_w)]\n",
    "    \n",
    "            loss = self.loss(training_data)\n",
    "            log_file.write(\"Epoch %d: %f\\n\" % (j, loss))\n",
    "            log_file.flush()  # Flush the file to disk after writing each line\n",
    "            losses.append(loss)\n",
    "    \n",
    "            self.weights = [w-learning_rate*ew for w, ew in zip(self.weights, error_w)]\n",
    "            self.biases = [b-learning_rate*eb for b, eb in zip(self.biases, error_b)]                \n",
    "    \n",
    "        log_file.close()\n",
    "\n",
    "        if save:\n",
    "            self.save(\"model.txt\")\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        error_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        error_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        activation = x\n",
    "        activations = [x]\n",
    "        zs = []\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = (self.func).activate(z)\n",
    "            activations.append(activation)\n",
    "        delta = (self.cost).delta(zs[-1], activations[-1], y)\n",
    "        error_b[-1] = delta\n",
    "        error_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = (self.func).delta(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            error_b[-l] = delta\n",
    "            error_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (error_b, error_w)\n",
    "    \n",
    "    def loss(self, data):\n",
    "        loss = 0\n",
    "        for x, y in data:\n",
    "            a = self.feedforward(x)\n",
    "            # if convert: y = vectorized_result(y)\n",
    "            loss += self.cost.fn(a, y)/len(data)\n",
    "        return loss\n",
    "    \n",
    "    def save(self, filename):\n",
    "        # save with the following format: sizes, weights, biases, cost, func line by line\n",
    "        with open(filename, \"w\") as f:\n",
    "            f.write(\"%s\\n\" % self.sizes)\n",
    "            for w in self.weights:\n",
    "                for row in w:\n",
    "                    f.write(\"%s\\n\" % row)\n",
    "            for b in self.biases:\n",
    "                for row in b:\n",
    "                    f.write(\"%s\\n\" % row)\n",
    "            f.write(\"%s\\n\" % self.cost)\n",
    "            f.write(\"%s\\n\" % self.func)\n",
    "        f.close()\n",
    "\n",
    "    \n",
    "    def gradient_check(self, training_data):\n",
    "        epsilon = 1e-5\n",
    "        tolerance = 1e-7\n",
    "        \n",
    "        for x, y in training_data:\n",
    "            backprop_b, backprop_w = self.backprop(x, y)\n",
    "\n",
    "            for i in range(len(self.biases)):\n",
    "                for j in range(len(self.biases[i])):\n",
    "                    original_bias = self.biases[i][j]\n",
    "                    self.biases[i][j] += epsilon\n",
    "                    loss_plus = self.loss([(x, y)])\n",
    "                    self.biases[i][j] -= 2*epsilon\n",
    "                    loss_minus = self.loss([(x, y)])\n",
    "                    self.biases[i][j] = original_bias\n",
    "                    numerical_gradient = (loss_plus - loss_minus) / (2*epsilon)\n",
    "\n",
    "                    if abs(numerical_gradient - backprop_b[i][j]) > tolerance:\n",
    "                        print(\"Gradient check failed for bias[%d][%d]: %f vs %f\" % (i, j, numerical_gradient, backprop_b[i][j]))\n",
    "\n",
    "            for i in range(len(self.weights)):\n",
    "                for j in range(len(self.weights[i])):\n",
    "                    for k in range(len(self.weights[i][j])):\n",
    "                        original_weight = self.weights[i][j][k]\n",
    "                        self.weights[i][j][k] += epsilon\n",
    "                        loss_plus = self.loss([(x, y)])\n",
    "                        self.weights[i][j][k] -= 2*epsilon\n",
    "                        loss_minus = self.loss([(x, y)])\n",
    "                        self.weights[i][j][k] = original_weight\n",
    "                        numerical_gradient = (loss_plus - loss_minus) / (2*epsilon)\n",
    "\n",
    "                        if abs(numerical_gradient - backprop_w[i][j][k]) > tolerance:\n",
    "                            print(\"Gradient check failed for weight[%d][%d][%d]: %f vs %f\" % (i, j, k, numerical_gradient, backprop_w[i][j][k]))\n",
    "\n",
    "def vectorized_result(j):\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79eb3e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_network():\n",
    "    file = open(\"network.txt\", \"r\")\n",
    "    # format: sizes, cost function, activation function each on a separate line \n",
    "    sizes = list(map(int, file.readline().split()))\n",
    "    cost_fn = file.readline().strip()\n",
    "    activation_fn = file.readline().strip()\n",
    "    net = Network(sizes, cost=cost_fn, func=activation_fn)\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0857b8b-93a2-429f-b864-12c518f29708",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
